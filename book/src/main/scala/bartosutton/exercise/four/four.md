# Chapter 4

All images are generated using this library.

## Figure 4.1

The data for column 1 is generated by the function `figure4dot1column1`. 
The iterative policy evaluation is based on the pseudocode in the section 
`Iterative Policy Evaluation, for estimating V≈ vπ` with a couple of 
modifications. First, it uses the "in place" version discussed in §4.1. 
Second, we check to see if each delta is below our threshold before stopping.

## Exercise 4.1

We want to know the value of q<sub>π</sub>(11, down) and q<sub>π</sub>(7, 
down).

Notice that we only have V<sub>π</sub>, not q<sub>π</sub>. However, 
q<sub>π</sub>(s, a) = V<sub>π</sub>(s'). That is, the value of the action-
value function is the value of the state that the agent ends up in (s') because 
of taking an action (a) from a state (s). So, for the first question, we can 
just ask what is the value of the terminal state V(T), and for the second 
question, we can ask what is the value of V(11).

A1: q<sub>π</sub>(11, down) = 0
A2: q<sub>π</sub>(7, down) = -14

## Exercise 4.2

If we added a new state, 15, what would the value of that state be if it had 
equiprobable transition probabilities and left took it to 12, up took it to 
13, right took it to 14, and down took it to 15, where 15 is unreachable from 
any other state?

We start with
V<sub>π</sub>(15) = 0.25 * -22 + 0.25 * -20 + 0.25 * -14 + 0.25 * -1

But notice the last term is -1. This needs to be updated to the new 
value of state 15, i.e., -14.25. Once it's updated, we need to update it 
again until the number stabilizes. Eventually we get to about -18.7.

V<sub>π</sub>(15) = -19

What if 15 is reachable from 13 via the down action?

One thing to consider is that this should change the value of state 13. 
(13, down) used to result in state 13 with a value of -20, but now it 
results in state 15, with a value of -19. Notice also, though, that given 
the other three states accesible from 13's values, it shouldn't change much.
Therefore, given that (15, up) results in state 13, state 15's value should 
not change much either.

In fact, if you compare the output of `figure4dot1column1()` and `exercise2b()` 
you'll notice it doesn't change significantly.

## Exercise 4.3

TODO

## Exercise 4.4

What is the bug in the pseudcode for policy iteration? Presumably this is 
referring to the fact that π(s) is probabilistic. Notice that in the function 
`policyImprovement()` we compare all of the possible actions of π and π'. 
This avoids the possibility that, when there are two or more equally greedy 
options, we compare two different actions that are optimal and determine that 
we have not yet converged.

## Exercise 4.5


